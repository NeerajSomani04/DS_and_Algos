English meaning of Asymptotic - A curve and a line that get closer but do not intersect

1. Asymptotic Analysis --> Used to evaluate the performance of an algorithm in terms of input size (we don’t measure the actual running time). We calculate, how does the time (or space) taken by an algorithm increases with the input size.

Linear Search (order of growth is linear) and other way is Binary Search (order of growth is logarithmic).
** we ignore constants in Asymptotic Analysis.

The main idea of asymptotic analysis is to have a measure of efficiency of algorithms that doesn’t depend on machine specific constants, and doesn’t require algorithms to be implemented and time taken by programs to be compared. 

2. (Worst, Average and Best Cases) -->  three cases to analyze an algorithm:

Worst Case Analysis (Usually always considered) --> means, what case/situation can cause max number of operation
we calculate upper bound on running time of an algorithm. We must know the case that causes maximum number of operations to be executed. 
for example, in linear search, worst case could be if value is not present in the array for which we are doing search as we need to iterate complete array. Hence, time complexity would be O(n).

Average Case Analysis (Sometimes considered)  --> we take all possible inputs and calculate computing time, then calculate average of this. Importantly, We must know (or predict) distribution of cases. 

Best Case Analysis (Bogus) --> we calculate lower bound on running time of an algorithm. We must know the case that causes minimum number of operations to be executed. for example, in linear search problem, element x is present at the first location.
The number of operations in the best case is constant, hence time complexity is O(1).

Although, For some algorithms, all the cases are asymptotically same, i.e., there are no worst and best cases.
For example, Merge Sort does Θ(nLogn) operations in all cases. 
but few other sorting algorithms there is difference in worst and best cases.

3 (Asymptotic Notations) --> 
   --> theta Notation --> bounds a functions from above and below, so it defines exact asymptotic behavior. means, need to calculate below and above behavoir seperately.
   --> Big O Notation ---> defines an upper bound of an algorithm, means including all below ones.
   --> Omega Notation --> provides an asymptotic lower bound. not so useful.

little o and little omega --> Little o provides strict upper bound (equality condition is removed from Big O) and little omega provides strict lower bound (equality condition removed from big omega)

4 (Analysis of Loops) --> 
Time complexity of a function (or set of statements) is considered as:
1. O(1) -->  If it doesn’t contain loop, recursion and call to any other non-constant time function.
For example, a swap() function, A loop or recursion that runs a constant number of times.
2. O(n) --> if the loop variables is incremented / decremented by a constant amount.
3. O(n^2) --> Time complexity of nested loops is equal to the number of times the innermost statement is executed. For example Selection sort and Insertion Sort have O(n2) time complexity.
4. O(log n) --> if the loop variables is divided / multiplied by a constant amount. For example, Binary Search.
5. O(log log n) --> if the loop variables is reduced / increased exponentially by a constant amount. 

**** How to combine time complexities of consecutive loops?
When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops.

**** How to calculate time complexity when there are many if, else statements inside loops?
 we need to consider worst case, Need to evaluate the situation when values in if-else conditions cause maximum number of statements to be executed.
   When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if else and other complex control statements.

*** How to calculate time complexity of recursive functions?
To calculate time complexity, we must know how to solve recurrences. which is the next topic discussed below:

Need to solve this Quiz once comfortable, https://www.geeksforgeeks.org/algorithms-gq/analysis-of-algorithms-gq/

5 (Solving Recurrences) --> 
 For example in Merge Sort, to sort a given array, we divide it in two halves and recursively repeat the process for the two halves.  Finally we merge the results. Time complexity of Merge Sort can be written as T(n) = 2T(n/2) + cn.
 
There are mainly three ways for solving recurrences.

1) Substitution Method: We make a guess for the solution and then we use mathematical induction to prove that the guess is correct or incorrect.

2) Recurrence Tree Method: In this method, we draw a recurrence tree and calculate the time taken by every level of tree. Finally, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. The pattern is typically an arithmetic or geometric series.

3) Master Method: is a direct way to get the solution. The master method works only for following type:
T(n) = aT(n/b) + f(n) where a >= 1 and b > 1 (If possible convert any other type to this type and it will work)

Notes: 
1) It is not necessary that a recurrence of the form T(n) = aT(n/b) + f(n) can be solved using Master Theorem. The given three cases have some gaps between them. For example, the recurrence T(n) = 2T(n/2) + n/Logn cannot be solved using master method.

2) Case 2 can be extended for f(n) = Θ(ncLogkn)
If f(n) = Θ(ncLogkn) for some constant k >= 0 and c = Logba, then T(n) = Θ(ncLogk+1n)

If needed, Practice question can be found here for Master Method: http://www.csd.uwo.ca/~moreno//CS424/Ressources/master.pdf

(Amortized Analysis Introduction) --> is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. we analyze a sequence of operations. 

meaning of Amortized --> reduce or extinguish (a debt) by money regularly put aside. (gradually write off the initial cost of (an asset).
most common examples are are Hash Tables, Disjoint Sets and Splay Trees.


(Space Complexity) --> total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input.

Auxiliary Space is the extra space or temporary space used by an algorithm.

For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criteria than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort and Heap Sort use O(1) auxiliary space. Space complexity of all these sorting algorithms is O(n) though.
